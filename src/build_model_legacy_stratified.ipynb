{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# Imports\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "import statsmodels.stats.proportion\n",
    "\n",
    "# seaborn\n",
    "import seaborn\n",
    "seaborn.set()\n",
    "seaborn.set_style(\"darkgrid\")\n",
    "\n",
    "# Project imports\n",
    "from model import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/workspace/scotus-predict-v2/env/lib/python3.4/site-packages/IPython/core/interactiveshell.py:2825: DtypeWarning: Columns (13) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  if self.run_code(code, result):\n"
     ]
    }
   ],
   "source": [
    "# Get raw data\n",
    "raw_data = get_raw_scdb_data(\"../data/input/SCDB_Legacy_01_justiceCentered_Citation.csv\")\n",
    "\n",
    "# Get feature data\n",
    "feature_df = preprocess_raw_data(raw_data, include_direction=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Output some diagnostics on features\n",
    "print(raw_data.shape)\n",
    "print(feature_df.shape)\n",
    "assert(raw_data.shape[0] == feature_df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Output basic quantities for sample\n",
    "print(pandas.DataFrame(raw_data[\"justice_outcome_disposition\"].value_counts()))\n",
    "print(pandas.DataFrame(raw_data[\"justice_outcome_disposition\"].value_counts(normalize=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Setup training time period\n",
    "dummy_window = 1000\n",
    "min_training_years = 50\n",
    "term_range = range(raw_data[\"term\"].min() + min_training_years,\n",
    "                   raw_data[\"term\"].max()+1)\n",
    "\n",
    "# Setting growing random forest parameters\n",
    "# Number of trees to grow per term\n",
    "trees_per_term = 2\n",
    "\n",
    "# Number of trees to begin with\n",
    "initial_trees = min_training_years * trees_per_term\n",
    "\n",
    "# Number of years between \"forest fires\"\n",
    "reset_interval = 9999\n",
    "\n",
    "# Setup model\n",
    "m = None\n",
    "term_count = 0\n",
    "\n",
    "for term in term_range:\n",
    "    # Diagnostic output\n",
    "    print(\"Term: {0}\".format(term))\n",
    "    term_count += 1\n",
    "    \n",
    "    # Setup train and test periods\n",
    "    train_index = (raw_data.loc[:, \"term\"] < term).values\n",
    "    test_index = (raw_data.loc[:, \"term\"] == term).values\n",
    "    \n",
    "    # Setup train data\n",
    "    feature_data_train = feature_df.loc[train_index, :]\n",
    "    target_data_train = raw_data.loc[train_index, \"justice_outcome_disposition\"].astype(int).values\n",
    "\n",
    "    # Setup test data\n",
    "    feature_data_test = feature_df.loc[test_index, :]\n",
    "    target_data_test = raw_data.loc[test_index, \"justice_outcome_disposition\"].astype(int).values\n",
    "    \n",
    "    # Check if we should rebuild the model based on changing natural court\n",
    "    if term_count % reset_interval == 0:\n",
    "        # \"Forest fire;\" grow a new forest from scratch\n",
    "        print(\"Reset interval hit; rebuilding with {0} trees\".format(initial_trees + (term_count * trees_per_term)))\n",
    "        m = None\n",
    "    else:\n",
    "        # Check if the justice set has changed\n",
    "        if set(raw_data.loc[raw_data.loc[:, \"term\"] == (term-1), \"justice\"].unique()) != \\\n",
    "            set(raw_data.loc[raw_data.loc[:, \"term\"] == (term), \"justice\"].unique()):\n",
    "            # natural Court change; trigger forest fire\n",
    "            print(\"Natural court change; rebuilding with {0} trees\".format(initial_trees + (term_count * trees_per_term)))\n",
    "        \n",
    "            m = None\n",
    "                                              \n",
    "    # Build or grow a model depending on initial/reset condition\n",
    "    if not m:\n",
    "        # Grow an initial forest\n",
    "        m = sklearn.ensemble.RandomForestClassifier(n_estimators=initial_trees + (term_count * trees_per_term), \n",
    "                                                    #class_weight=\"balanced_subsample\",\n",
    "                                                    warm_start=True,\n",
    "                                                    n_jobs=-1)\n",
    "    else:\n",
    "        # Grow the forest by increasing the number of trees (requires warm_start=True)\n",
    "        m.set_params(n_estimators=initial_trees + (term_count * trees_per_term))\n",
    "\n",
    "    # Fit the forest model\n",
    "    m.fit(feature_data_train,\n",
    "          target_data_train)\n",
    "\n",
    "    # Fit the \"dummy\" model\n",
    "    d = sklearn.dummy.DummyClassifier(strategy=\"stratified\")\n",
    "    d.fit(feature_data_train.tail(dummy_window), pandas.Series(target_data_train).tail(dummy_window))\n",
    "    \n",
    "    # Perform forest predictions\n",
    "    raw_data.loc[test_index, \"rf_predicted\"] = m.predict(feature_data_test)\n",
    "    \n",
    "    # Store scores per class\n",
    "    scores = m.predict_proba(feature_data_test)\n",
    "    raw_data.loc[test_index, \"rf_predicted_score_other\"] = scores[:, 0]\n",
    "    raw_data.loc[test_index, \"rf_predicted_score_affirm\"] = scores[:, 1]\n",
    "    raw_data.loc[test_index, \"rf_predicted_score_reverse\"] = scores[:, 2]\n",
    "    \n",
    "    # Store dummy predictions\n",
    "    raw_data.loc[test_index, \"dummy_predicted\"] = d.predict(feature_data_test)\n",
    "    \n",
    "    #  Clear\n",
    "    del feature_data_train\n",
    "    del feature_data_test\n",
    "    del target_data_train\n",
    "    del target_data_test\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Evaluation range\n",
    "evaluation_index = raw_data.loc[:, \"term\"].isin(term_range)\n",
    "target_actual = raw_data.loc[evaluation_index, \"justice_outcome_disposition\"]\n",
    "target_predicted = raw_data.loc[evaluation_index, \"rf_predicted\"]\n",
    "target_dummy = raw_data.loc[evaluation_index, \"dummy_predicted\"]\n",
    "raw_data.loc[:, \"rf_correct\"] = numpy.nan\n",
    "raw_data.loc[:, \"dummy_correct\"] = numpy.nan\n",
    "raw_data.loc[evaluation_index, \"rf_correct\"] = (target_actual == target_predicted).astype(float)\n",
    "raw_data.loc[evaluation_index, \"dummy_correct\"] = (target_actual == target_dummy).astype(float)\n",
    "\n",
    "# Compare model\n",
    "print(\"RF model\")\n",
    "print(\"=\"*32)\n",
    "print(sklearn.metrics.classification_report(target_actual, target_predicted))\n",
    "print(sklearn.metrics.confusion_matrix(target_actual, target_predicted))\n",
    "print(sklearn.metrics.accuracy_score(target_actual, target_predicted))\n",
    "print(\"=\"*32)\n",
    "print(\"\")\n",
    "\n",
    "# Dummy model\n",
    "print(\"Dummy model\")\n",
    "print(\"=\"*32)\n",
    "print(sklearn.metrics.classification_report(target_actual, target_dummy))\n",
    "print(sklearn.metrics.confusion_matrix(target_actual, target_dummy))\n",
    "print(sklearn.metrics.accuracy_score(target_actual, target_dummy))\n",
    "print(\"=\"*32)\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Setup time series\n",
    "rf_correct_ts = raw_data.loc[evaluation_index, :].groupby(\"term\")[\"rf_correct\"].mean()\n",
    "dummy_correct_ts = raw_data.loc[evaluation_index, :].groupby(\"term\")[\"dummy_correct\"].mean()\n",
    "\n",
    "# Plot all accuracies\n",
    "f = plt.figure(figsize=(16, 12))\n",
    "plt.plot(rf_correct_ts.index, rf_correct_ts,\n",
    "         marker='o', alpha=0.75)\n",
    "plt.plot(dummy_correct_ts.index, dummy_correct_ts,\n",
    "         marker='>', alpha=0.75)\n",
    "plt.legend(('Random forest', 'Dummy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Setup time series\n",
    "rf_spread_ts = rf_correct_ts - dummy_correct_ts\n",
    "\n",
    "# Plot all accuracies\n",
    "f = plt.figure(figsize=(16, 12))\n",
    "plt.bar(rf_spread_ts.index, rf_spread_ts,\n",
    "        alpha=0.75)\n",
    "plt.xlabel(\"Term\")\n",
    "plt.ylabel(\"Spread (%)\")\n",
    "plt.title(\"Spread over dummy model for justice accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Feature importance\n",
    "feature_importance_df = pandas.DataFrame(list(zip(feature_df.columns, m.feature_importances_)),\n",
    "                                         columns=[\"feature\", \"importance\"])\n",
    "feature_importance_df.sort_values([\"importance\"], ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Output stats\n",
    "print(\"t-test:\")\n",
    "print(\"Uncalibrated:\")\n",
    "print(scipy.stats.ttest_rel(rf_correct_ts.values,\n",
    "                   dummy_correct_ts.values))\n",
    "\n",
    "print(\"=\" * 16)\n",
    "print(\"ranksum-test:\")\n",
    "print(\"Uncalibrated:\")\n",
    "print(scipy.stats.ranksums(rf_correct_ts.values,\n",
    "                   dummy_correct_ts.values))\n",
    "\n",
    "print(\"=\" * 16)\n",
    "print(\"Binomial:\")\n",
    "print(statsmodels.stats.proportion.binom_test(raw_data.loc[evaluation_index, \"rf_correct\"].sum(),\n",
    "                                              raw_data.loc[evaluation_index, \"rf_correct\"].shape[0],\n",
    "                                              raw_data.loc[evaluation_index, \"dummy_correct\"].mean(),\n",
    "                                              alternative=\"larger\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get case-level prediction\n",
    "#scdb_data.loc[evaluation_index, \"rf_predicted_case\"] = \n",
    "rf_predicted_case = pandas.DataFrame(raw_data.loc[evaluation_index, :]\\\n",
    "    .groupby([\"docketId\"])[\"rf_predicted\"]\\\n",
    "    .agg(lambda x: x.value_counts().index[0]))\n",
    "rf_predicted_case.columns = [\"rf_predicted_case\"]\n",
    "\n",
    "dummy_predicted_case = pandas.DataFrame(raw_data.loc[evaluation_index, :]\\\n",
    "    .groupby([\"docketId\"])[\"dummy_predicted\"]\\\n",
    "    .agg(lambda x: x.value_counts().index[0]))\n",
    "dummy_predicted_case.columns = [\"dummy_predicted_case\"]\n",
    "\n",
    "# Set DFs\n",
    "rf_predicted_case = raw_data[[\"docketId\", \"case_outcome_disposition\", \"rf_predicted\"]].join(rf_predicted_case, on=\"docketId\")\n",
    "dumy_predicted_case = raw_data[[\"docketId\", \"dummy_predicted\"]].join(dummy_predicted_case, on=\"docketId\")\n",
    "\n",
    "raw_data.loc[:, \"rf_predicted_case\"] = rf_predicted_case\n",
    "raw_data.loc[:, \"dummy_predicted_case\"] = dumy_predicted_case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Output case distribution\n",
    "case_outcomes = raw_data.groupby([\"docketId\"])[\"case_outcome_disposition\"].apply(lambda x: x.mode())\n",
    "case_outcomes = case_outcomes.apply(lambda x: int(x) if type(x) in [numpy.float64] else None)\n",
    "print(case_outcomes.value_counts())\n",
    "print(case_outcomes.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Output comparison\n",
    "# Evaluation range\n",
    "evaluation_index = raw_data.loc[:, \"term\"].isin(term_range) & -raw_data.loc[:, \"case_outcome_disposition\"].isnull()\n",
    "target_actual = raw_data.loc[evaluation_index, \"case_outcome_disposition\"]\n",
    "target_predicted = raw_data.loc[evaluation_index, \"rf_predicted_case\"]\n",
    "target_dummy = raw_data.loc[evaluation_index, \"dummy_predicted_case\"]\n",
    "\n",
    "raw_data.loc[:, \"rf_correct_case\"] = numpy.nan\n",
    "raw_data.loc[:, \"dummy_correct_case\"] = numpy.nan\n",
    "raw_data.loc[evaluation_index, \"rf_correct_case\"] = (target_actual == target_predicted).astype(float)\n",
    "raw_data.loc[evaluation_index, \"dummy_correct_case\"] = (target_actual == target_dummy).astype(float)\n",
    "\n",
    "# Compare model\n",
    "print(\"RF model\")\n",
    "print(\"=\"*32)\n",
    "print(sklearn.metrics.classification_report(target_actual, target_predicted))\n",
    "print(sklearn.metrics.confusion_matrix(target_actual, target_predicted))\n",
    "print(sklearn.metrics.accuracy_score(target_actual, target_predicted))\n",
    "print(\"=\"*32)\n",
    "print(\"\")\n",
    "\n",
    "# Dummy model\n",
    "print(\"Dummy model\")\n",
    "print(\"=\"*32)\n",
    "print(sklearn.metrics.classification_report(target_actual, target_dummy))\n",
    "print(sklearn.metrics.confusion_matrix(target_actual, target_dummy))\n",
    "print(sklearn.metrics.accuracy_score(target_actual, target_dummy))\n",
    "print(\"=\"*32)\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Setup time series\n",
    "rf_correct_case_ts = raw_data.loc[evaluation_index, :].groupby(\"term\")[\"rf_correct_case\"].mean()\n",
    "dummy_correct_case_ts = raw_data.loc[evaluation_index, :].groupby(\"term\")[\"dummy_correct_case\"].mean()\n",
    "\n",
    "# Plot all accuracies\n",
    "f = plt.figure(figsize=(16, 12))\n",
    "plt.plot(rf_correct_case_ts.index, rf_correct_case_ts,\n",
    "         marker='o', alpha=0.75)\n",
    "plt.plot(dummy_correct_case_ts.index, dummy_correct_case_ts,\n",
    "         marker='>', alpha=0.75)\n",
    "plt.legend(('Random forest', 'Dummy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Setup time series\n",
    "rf_spread_case_ts = rf_correct_case_ts - dummy_correct_case_ts\n",
    "\n",
    "# Plot all accuracies\n",
    "f = plt.figure(figsize=(16, 12))\n",
    "plt.bar(rf_spread_case_ts.index, rf_spread_case_ts,\n",
    "        alpha=0.75)\n",
    "plt.xlabel(\"Term\")\n",
    "plt.ylabel(\"Spread (%)\")\n",
    "plt.title(\"Spread over dummy model for case accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Setup time series\n",
    "rf_spread_case_dir_ts = pandas.expanding_sum(numpy.sign(rf_spread_case_ts))\n",
    "\n",
    "# Plot all accuracies\n",
    "f = plt.figure(figsize=(16, 12))\n",
    "plt.plot(rf_spread_case_dir_ts.index, rf_spread_case_dir_ts,\n",
    "        alpha=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Output stats\n",
    "print(\"t-test:\")\n",
    "print(\"Uncalibrated:\")\n",
    "print(scipy.stats.ttest_rel(rf_correct_case_ts.values,\n",
    "                   dummy_correct_case_ts.values))\n",
    "\n",
    "print(\"=\" * 16)\n",
    "print(\"ranksum-test:\")\n",
    "print(\"Uncalibrated:\")\n",
    "print(scipy.stats.ranksums(rf_correct_case_ts.values,\n",
    "                   dummy_correct_case_ts.values))\n",
    "\n",
    "print(\"=\" * 16)\n",
    "print(\"Binomial:\")\n",
    "case_accuracy_data = raw_data.loc[evaluation_index, [\"docketId\", \"rf_correct_case\", \"dummy_correct_case\"]].drop_duplicates()\n",
    "print(statsmodels.stats.proportion.binom_test(case_accuracy_data[\"rf_correct_case\"].sum(),\n",
    "                                              case_accuracy_data[\"rf_correct_case\"].shape[0],\n",
    "                                              case_accuracy_data[\"dummy_correct_case\"].mean(),\n",
    "                                              alternative=\"larger\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "raw_data.loc[raw_data.loc[:, \"caseName\"] == \"MIRANDA v. ARIZONA\", \n",
    "              [\"caseName\", \"justiceName\", \"case_outcome_disposition\", \"justice_outcome_disposition\",\n",
    "              \"rf_predicted\", \"rf_predicted_score_affirm\", \"rf_predicted_score_reverse\", \"rf_correct_case\", \"dummy_correct_case\"]].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
